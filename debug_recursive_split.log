[R_SPLIT_DEBUG] Depth 0: Processing lines 0-327. Input text: ## 7. General Deep Crawl Configuration and Usage

### 7.1. Example: Deep cr...__ == "__main__":
    asyncio.run(compare_deep_crawl_strategies())
```

---
[R_SPLIT_DEBUG] Found 7 local headings: [('General Deep Crawl Configuration and Usage', 2), ('Example: Deep crawling a site that relies heavily on JavaScript for link generation.', 3), ('Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', 3), ('Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', 3), ('Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', 3), ('Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', 3), ('Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('General Deep Crawl Configuration and Usage', 0)]
[R_SPLIT_DEBUG] Single heading 'General Deep Crawl Configuration and Usage' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 7. General Deep Crawl Configuration and Usage

### 7.1. Example: Deep cr...__ == "__main__":
    asyncio.run(compare_deep_crawl_strategies())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 14. Snippets: ['## 7. General Deep Crawl Configuration and Usage\n\n### 7.1. Example: Deep cr...ates the setup. A real JS-heavy site would be needed for full verification.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\nif __name__ == "__main__":\n    asyncio.run(deep_crawl_js_heavy_site())\n```', '### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\n\nif __name__ == "__main__":\n    asyncio.run(deep_crawl_with_configs())\n```', '### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.\nA robust deep crawl should handle partial failures gracefully.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(deep_crawl_handling_failures())\n```', '### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', '```python\nimport asyncio\nimport logging\nfrom crawl4ai import AsyncWebCrawle...\nif __name__ == "__main__":\n    asyncio.run(deep_crawl_custom_logger())\n```', '### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":\n    asyncio.run(deep_crawl_from_local_file())\n```', '### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":\n    asyncio.run(compare_deep_crawl_strategies())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 14 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-5): ## 7. General Deep Crawl Configuration and Usage

### 7.1. Example: Deep cr...ates the setup. A real JS-heavy site would be needed for full verification.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-5. Input text: ## 7. General Deep Crawl Configuration and Usage

### 7.1. Example: Deep cr...ates the setup. A real JS-heavy site would be needed for full verification.
  [R_SPLIT_DEBUG] Found 2 local headings: [('General Deep Crawl Configuration and Usage', 2), ('Example: Deep crawling a site that relies heavily on JavaScript for link generation.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('General Deep Crawl Configuration and Usage', 0)]
  [R_SPLIT_DEBUG] Single heading 'General Deep Crawl Configuration and Usage' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (51 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 5-53): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(deep_crawl_js_heavy_site())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 5-53. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(deep_crawl_js_heavy_site())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (558 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 53-58): ### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 53-58. Input text: ### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', 55)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.' (lines 55-58): ### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 55-58. Input text: ### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', 55)]
    [R_SPLIT_DEBUG] Single heading 'Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.' at start_line 55. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (83 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) ...nd `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.', h='Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.', lines 55-58)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 58-111): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(deep_crawl_with_configs())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 58-111. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(deep_crawl_with_configs())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (638 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 111-116): ### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.
A robust deep crawl should handle partial failures gracefully.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 111-116. Input text: ### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.
A robust deep crawl should handle partial failures gracefully.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', 113)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.' (lines 113-116): ### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.
A robust deep crawl should handle partial failures gracefully.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 113-116. Input text: ### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.
A robust deep crawl should handle partial failures gracefully.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', 113)]
    [R_SPLIT_DEBUG] Single heading 'Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.' at start_line 113. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (38 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 7.3. Example: Iterating through deep crawl results and handling cases w...iltered out.\nA robust deep crawl should handle partial failures gracefully.', h='Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.', lines 113-116)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 116-177): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(deep_crawl_handling_failures())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 116-177. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(deep_crawl_handling_failures())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (782 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 177-181): ### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 177-181. Input text: ### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', 179)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.' (lines 179-181): ### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 179-181. Input text: ### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', 179)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.' at start_line 179. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (24 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', h='Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.', lines 179-181)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 181-207): ```python
import asyncio
import logging
from crawl4ai import AsyncWebCrawle...
if __name__ == "__main__":
    asyncio.run(deep_crawl_custom_logger())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 181-207. Input text: ```python
import asyncio
import logging
from crawl4ai import AsyncWebCrawle...
if __name__ == "__main__":
    asyncio.run(deep_crawl_custom_logger())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (310 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 207-211): ### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 207-211. Input text: ### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', 209)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.' (lines 209-211): ### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 209-211. Input text: ### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', 209)]
    [R_SPLIT_DEBUG] Single heading 'Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.' at start_line 209. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (27 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', h='Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.', lines 209-211)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 211-259): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(deep_crawl_from_local_file())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 211-259. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(deep_crawl_from_local_file())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (493 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 259-264): ### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 259-264. Input text: ### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', 261)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.' (lines 261-264): ### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 261-264. Input text: ### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', 261)]
    [R_SPLIT_DEBUG] Single heading 'Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.' at start_line 261. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (56 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCr...es with similar settings to highlight differences in traversal and results.', h='Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.', lines 261-264)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 264-324): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(compare_deep_crawl_strategies())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 264-324. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(compare_deep_crawl_strategies())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (668 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 324-326):
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 326-326): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 326-326. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 13 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-711. Input text: ## 5. Configuring Filters (`FilterChain`) for Deep Crawling

Filters allow ..._name__ == "__main__":
    asyncio.run(filter_chain_allow_empty())
```

---
[R_SPLIT_DEBUG] Found 27 local headings: [('Configuring Filters (`FilterChain`) for Deep Crawling', 2), ('`URLPatternFilter`', 3), ('Example: Using `URLPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).', 4), ('Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', 4), ('Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', 4), ('`DomainFilter`', 3), ('Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.', 4), ('Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', 4), ('Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', 4), ('Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', 4), ('`ContentTypeFilter`', 3), ('Example: Using `ContentTypeFilter` to allow only `text/html` pages.', 4), ('Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', 4), ('Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', 4), ('`URLFilter` (Simple exact match)', 3), ('Example: `URLFilter` to allow a specific list of exact URLs.', 4), ('Example: `URLFilter` to block a specific list of exact URLs.', 4), ('`ContentRelevanceFilter`', 3), ('Example: Setting up `ContentRelevanceFilter` with target keywords (conceptual, focusing on setup).', 4), ('Example: `ContentRelevanceFilter` with a custom `threshold`.', 4), ('`SEOFilter`', 3), ('Example: Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).', 4), ('Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', 4), ('`FilterChain`', 3), ('Example: Combining `URLPatternFilter` (allow `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.', 4), ('Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', 4), ('Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', 4)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Configuring Filters (`FilterChain`) for Deep Crawling', 0)]
[R_SPLIT_DEBUG] Single heading 'Configuring Filters (`FilterChain`) for Deep Crawling' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 5. Configuring Filters (`FilterChain`) for Deep Crawling

Filters allow ..._name__ == "__main__":
    asyncio.run(filter_chain_allow_empty())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 40. Snippets: ['## 5. Configuring Filters (`FilterChain`) for Deep Crawling\n\nFilters allow ...LPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo....")\n\nif __name__ == "__main__":\n    asyncio.run(filter_allow_pattern())\n```', '#### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo....")\n\nif __name__ == "__main__":\n    asyncio.run(filter_block_pattern())\n```', '#### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":\n    asyncio.run(filter_pattern_case_sensitivity())\n```', '### 5.2. `DomainFilter`\n\n#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...)\n\nif __name__ == "__main__":\n    asyncio.run(filter_allowed_domains())\n```', '#### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...)\n\nif __name__ == "__main__":\n    asyncio.run(filter_blocked_domains())\n```', "#### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)", '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\n\nif __name__ == "__main__":\n    asyncio.run(filter_allow_subdomains())\n```', "#### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)", '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":\n    asyncio.run(filter_disallow_subdomains())\n```', '### 5.3. `ContentTypeFilter`\n\n#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...)\n\nif __name__ == "__main__":\n    asyncio.run(filter_allow_html_only())\n```', '#### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).\n(Conceptual, as MOCK_SITE_DATA only has html/pdf)', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":\n    asyncio.run(filter_allow_multiple_types())\n```', '#### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...PDF).")\n\nif __name__ == "__main__":\n    asyncio.run(filter_block_pdf())\n```', '### 5.4. `URLFilter` (Simple exact match)\n\n#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\n\nif __name__ == "__main__":\n    asyncio.run(filter_allow_exact_urls())\n```', '#### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\n\nif __name__ == "__main__":\n    asyncio.run(filter_block_exact_urls())\n```', '### 5.5. `ContentRelevanceFilter`\nThis filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":\n    asyncio.run(setup_content_relevance_filter())\n```', '#### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.', '```python\nimport asyncio\nfrom crawl4ai import ContentRelevanceFilter, LLMCo...__ == "__main__":\n    asyncio.run(content_relevance_custom_threshold())\n```', '### 5.6. `SEOFilter`\nThis filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).', '```python\nimport asyncio\nfrom crawl4ai import SEOFilter\n\nasync def setup_ba...)\n\nif __name__ == "__main__":\n    asyncio.run(setup_basic_seo_filter())\n```', '#### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', '```python\nimport asyncio\nfrom crawl4ai import SEOFilter\n\nasync def setup_cu...\n\nif __name__ == "__main__":\n    asyncio.run(setup_custom_seo_filter())\n```', '### 5.7. `FilterChain`\n\n#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\nif __name__ == "__main__":\n    asyncio.run(filter_chain_combination())\n```', '#### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\n\nif __name__ == "__main__":\n    asyncio.run(filter_chain_with_stats())\n```', '#### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\nif __name__ == "__main__":\n    asyncio.run(filter_chain_allow_empty())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 40 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-8): ## 5. Configuring Filters (`FilterChain`) for Deep Crawling

Filters allow ...LPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-8. Input text: ## 5. Configuring Filters (`FilterChain`) for Deep Crawling

Filters allow ...LPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).
  [R_SPLIT_DEBUG] Found 3 local headings: [('Configuring Filters (`FilterChain`) for Deep Crawling', 2), ('`URLPatternFilter`', 3), ('Example: Using `URLPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Configuring Filters (`FilterChain`) for Deep Crawling', 0)]
  [R_SPLIT_DEBUG] Single heading 'Configuring Filters (`FilterChain`) for Deep Crawling' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (93 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 8-36): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo....")

if __name__ == "__main__":
    asyncio.run(filter_allow_pattern())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 8-36. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo....")

if __name__ == "__main__":
    asyncio.run(filter_allow_pattern())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (319 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 36-40): #### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 36-40. Input text: #### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', 38)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).' (lines 38-40): #### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 38-40. Input text: #### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', 38)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).' at start_line 38. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (37 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', h='Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).', lines 38-40)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 40-68): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo....")

if __name__ == "__main__":
    asyncio.run(filter_block_pattern())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 40-68. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo....")

if __name__ == "__main__":
    asyncio.run(filter_block_pattern())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (345 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 68-72): #### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 68-72. Input text: #### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', 70)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.' (lines 70-72): #### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 70-72. Input text: #### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', 70)]
    [R_SPLIT_DEBUG] Single heading 'Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.' at start_line 70. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (28 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', h='Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.', lines 70-72)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 72-126): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":
    asyncio.run(filter_pattern_case_sensitivity())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 72-126. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":
    asyncio.run(filter_pattern_case_sensitivity())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (749 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 126-132): ### 5.2. `DomainFilter`

#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 126-132. Input text: ### 5.2. `DomainFilter`

#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`DomainFilter`', 3), ('Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`DomainFilter`', 128)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`DomainFilter`' (lines 128-132): ### 5.2. `DomainFilter`

#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 128-132. Input text: ### 5.2. `DomainFilter`

#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`DomainFilter`', 3), ('Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`DomainFilter`', 128)]
    [R_SPLIT_DEBUG] Single heading '`DomainFilter`' at start_line 128. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (40 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.2. `DomainFilter`\n\n#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.', h='`DomainFilter`', lines 128-132)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 132-160): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_allowed_domains())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 132-160. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_allowed_domains())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (343 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 160-164): #### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 160-164. Input text: #### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', 162)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.' (lines 162-164): #### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 162-164. Input text: #### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', 162)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.' at start_line 162. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (26 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', h='Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.', lines 162-164)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 164-191): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_blocked_domains())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 164-191. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_blocked_domains())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (294 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 191-196): #### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
  [R_SPLIT_DEBUG] Depth 1: Processing lines 191-196. Input text: #### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', 193)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).' (lines 193-196): #### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
    [R_SPLIT_DEBUG] Depth 2: Processing lines 193-196. Input text: #### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', 193)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).' at start_line 193. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (48 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)', h='Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).', lines 193-196)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 196-226): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_allow_subdomains())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 196-226. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_allow_subdomains())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (346 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 226-231): #### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
  [R_SPLIT_DEBUG] Depth 1: Processing lines 226-231. Input text: #### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', 228)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).' (lines 228-231): #### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
    [R_SPLIT_DEBUG] Depth 2: Processing lines 228-231. Input text: #### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', 228)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).' at start_line 228. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (49 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`all...ceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)', h='Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).', lines 228-231)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 231-257): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(filter_disallow_subdomains())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 231-257. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(filter_disallow_subdomains())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (319 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 257-263): ### 5.3. `ContentTypeFilter`

#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 257-263. Input text: ### 5.3. `ContentTypeFilter`

#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentTypeFilter`', 3), ('Example: Using `ContentTypeFilter` to allow only `text/html` pages.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentTypeFilter`', 259)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`ContentTypeFilter`' (lines 259-263): ### 5.3. `ContentTypeFilter`

#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 259-263. Input text: ### 5.3. `ContentTypeFilter`

#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentTypeFilter`', 3), ('Example: Using `ContentTypeFilter` to allow only `text/html` pages.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentTypeFilter`', 259)]
    [R_SPLIT_DEBUG] Single heading '`ContentTypeFilter`' at start_line 259. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (34 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.3. `ContentTypeFilter`\n\n#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.', h='`ContentTypeFilter`', lines 259-263)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 15 (lines approx 263-291): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_allow_html_only())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 263-291. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)

if __name__ == "__main__":
    asyncio.run(filter_allow_html_only())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (331 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 16 (lines approx 291-296): #### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).
(Conceptual, as MOCK_SITE_DATA only has html/pdf)
  [R_SPLIT_DEBUG] Depth 1: Processing lines 291-296. Input text: #### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).
(Conceptual, as MOCK_SITE_DATA only has html/pdf)
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', 293)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).' (lines 293-296): #### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).
(Conceptual, as MOCK_SITE_DATA only has html/pdf)
    [R_SPLIT_DEBUG] Depth 2: Processing lines 293-296. Input text: #### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).
(Conceptual, as MOCK_SITE_DATA only has html/pdf)
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', 293)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).' at start_line 293. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (47 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types...ml`, `application/json`).\n(Conceptual, as MOCK_SITE_DATA only has html/pdf)', h='Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).', lines 293-296)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 17 (lines approx 296-341): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(filter_allow_multiple_types())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 296-341. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(filter_allow_multiple_types())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (566 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 18 (lines approx 341-345): #### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 341-345. Input text: #### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', 343)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).' (lines 343-345): #### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 343-345. Input text: #### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', 343)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).' at start_line 343. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (30 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', h='Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).', lines 343-345)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 19 (lines approx 345-372): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...PDF).")

if __name__ == "__main__":
    asyncio.run(filter_block_pdf())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 345-372. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...PDF).")

if __name__ == "__main__":
    asyncio.run(filter_block_pdf())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (314 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 20 (lines approx 372-378): ### 5.4. `URLFilter` (Simple exact match)

#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 372-378. Input text: ### 5.4. `URLFilter` (Simple exact match)

#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`URLFilter` (Simple exact match)', 3), ('Example: `URLFilter` to allow a specific list of exact URLs.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`URLFilter` (Simple exact match)', 374)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`URLFilter` (Simple exact match)' (lines 374-378): ### 5.4. `URLFilter` (Simple exact match)

#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 374-378. Input text: ### 5.4. `URLFilter` (Simple exact match)

#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`URLFilter` (Simple exact match)', 3), ('Example: `URLFilter` to allow a specific list of exact URLs.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`URLFilter` (Simple exact match)', 374)]
    [R_SPLIT_DEBUG] Single heading '`URLFilter` (Simple exact match)' at start_line 374. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (38 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.4. `URLFilter` (Simple exact match)\n\n#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.', h='`URLFilter` (Simple exact match)', lines 374-378)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 21 (lines approx 378-411): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_allow_exact_urls())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 378-411. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_allow_exact_urls())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (385 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 22 (lines approx 411-415): #### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 411-415. Input text: #### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `URLFilter` to block a specific list of exact URLs.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `URLFilter` to block a specific list of exact URLs.', 413)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `URLFilter` to block a specific list of exact URLs.' (lines 413-415): #### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 413-415. Input text: #### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `URLFilter` to block a specific list of exact URLs.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `URLFilter` to block a specific list of exact URLs.', 413)]
    [R_SPLIT_DEBUG] Single heading 'Example: `URLFilter` to block a specific list of exact URLs.' at start_line 413. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (23 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.', h='Example: `URLFilter` to block a specific list of exact URLs.', lines 413-415)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 23 (lines approx 415-445): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_block_exact_urls())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 415-445. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_block_exact_urls())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (325 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 24 (lines approx 445-452): ### 5.5. `ContentRelevanceFilter`
This filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 445-452. Input text: ### 5.5. `ContentRelevanceFilter`
This filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).
  [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentRelevanceFilter`', 3), ('Example: Setting up `ContentRelevanceFilter` with target keywords (conceptual, focusing on setup).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentRelevanceFilter`', 447)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`ContentRelevanceFilter`' (lines 447-452): ### 5.5. `ContentRelevanceFilter`
This filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 447-452. Input text: ### 5.5. `ContentRelevanceFilter`
This filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).
    [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentRelevanceFilter`', 3), ('Example: Setting up `ContentRelevanceFilter` with target keywords (conceptual, focusing on setup).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentRelevanceFilter`', 447)]
    [R_SPLIT_DEBUG] Single heading '`ContentRelevanceFilter`' at start_line 447. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (66 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.5. `ContentRelevanceFilter`\nThis filter uses an LLM to determine rele...ntentRelevanceFilter` with target keywords (conceptual, focusing on setup).', h='`ContentRelevanceFilter`', lines 447-452)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 25 (lines approx 452-497): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(setup_content_relevance_filter())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 452-497. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(setup_content_relevance_filter())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (532 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 26 (lines approx 497-501): #### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 497-501. Input text: #### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `ContentRelevanceFilter` with a custom `threshold`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `ContentRelevanceFilter` with a custom `threshold`.', 499)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `ContentRelevanceFilter` with a custom `threshold`.' (lines 499-501): #### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 499-501. Input text: #### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `ContentRelevanceFilter` with a custom `threshold`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `ContentRelevanceFilter` with a custom `threshold`.', 499)]
    [R_SPLIT_DEBUG] Single heading 'Example: `ContentRelevanceFilter` with a custom `threshold`.' at start_line 499. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (22 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.', h='Example: `ContentRelevanceFilter` with a custom `threshold`.', lines 499-501)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 27 (lines approx 501-528): ```python
import asyncio
from crawl4ai import ContentRelevanceFilter, LLMCo...__ == "__main__":
    asyncio.run(content_relevance_custom_threshold())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 501-528. Input text: ```python
import asyncio
from crawl4ai import ContentRelevanceFilter, LLMCo...__ == "__main__":
    asyncio.run(content_relevance_custom_threshold())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (233 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 28 (lines approx 528-535): ### 5.6. `SEOFilter`
This filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 528-535. Input text: ### 5.6. `SEOFilter`
This filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).
  [R_SPLIT_DEBUG] Found 2 local headings: [('`SEOFilter`', 3), ('Example: Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`SEOFilter`', 530)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`SEOFilter`' (lines 530-535): ### 5.6. `SEOFilter`
This filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 530-535. Input text: ### 5.6. `SEOFilter`
This filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).
    [R_SPLIT_DEBUG] Found 2 local headings: [('`SEOFilter`', 3), ('Example: Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`SEOFilter`', 530)]
    [R_SPLIT_DEBUG] Single heading '`SEOFilter`' at start_line 530. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (54 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.6. `SEOFilter`\nThis filter checks for common SEO issues. The example ... Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).', h='`SEOFilter`', lines 530-535)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 29 (lines approx 535-555): ```python
import asyncio
from crawl4ai import SEOFilter

async def setup_ba...)

if __name__ == "__main__":
    asyncio.run(setup_basic_seo_filter())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 535-555. Input text: ```python
import asyncio
from crawl4ai import SEOFilter

async def setup_ba...)

if __name__ == "__main__":
    asyncio.run(setup_basic_seo_filter())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (185 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 30 (lines approx 555-559): #### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 555-559. Input text: #### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', 557)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).' (lines 557-559): #### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 557-559. Input text: #### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', 557)]
    [R_SPLIT_DEBUG] Single heading 'Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).' at start_line 557. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (40 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_titl...`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', h='Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).', lines 557-559)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 31 (lines approx 559-582): ```python
import asyncio
from crawl4ai import SEOFilter

async def setup_cu...

if __name__ == "__main__":
    asyncio.run(setup_custom_seo_filter())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 559-582. Input text: ```python
import asyncio
from crawl4ai import SEOFilter

async def setup_cu...

if __name__ == "__main__":
    asyncio.run(setup_custom_seo_filter())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (217 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 32 (lines approx 582-588): ### 5.7. `FilterChain`

#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 582-588. Input text: ### 5.7. `FilterChain`

#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`FilterChain`', 3), ('Example: Combining `URLPatternFilter` (allow `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`FilterChain`', 584)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`FilterChain`' (lines 584-588): ### 5.7. `FilterChain`

#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 584-588. Input text: ### 5.7. `FilterChain`

#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`FilterChain`', 3), ('Example: Combining `URLPatternFilter` (allow `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`FilterChain`', 584)]
    [R_SPLIT_DEBUG] Single heading '`FilterChain`' at start_line 584. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (50 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 5.7. `FilterChain`\n\n#### 5.7.1. Example: Combining `URLPatternFilter` (... `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.', h='`FilterChain`', lines 584-588)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 33 (lines approx 588-631): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(filter_chain_combination())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 588-631. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(filter_chain_combination())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (558 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 34 (lines approx 631-635): #### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 631-635. Input text: #### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', 633)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.' (lines 633-635): #### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 633-635. Input text: #### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', 633)]
    [R_SPLIT_DEBUG] Single heading 'Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.' at start_line 633. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (29 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', h='Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.', lines 633-635)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 35 (lines approx 635-671): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_chain_with_stats())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 635-671. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...

if __name__ == "__main__":
    asyncio.run(filter_chain_with_stats())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (456 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 36 (lines approx 671-676): #### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 671-676. Input text: #### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', 673)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.' (lines 673-676): #### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 673-676. Input text: #### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', 673)]
    [R_SPLIT_DEBUG] Single heading 'Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.' at start_line 673. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (68 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=...lt), an empty chain allows all URLs. If `False`, an empty chain blocks all.', h='Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.', lines 673-676)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 37 (lines approx 676-708): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(filter_chain_allow_empty())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 676-708. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(filter_chain_allow_empty())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (441 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 38 (lines approx 708-710):
[R_SPLIT_DEBUG] Recursing on delimiter part 39 (lines approx 710-710): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 710-710. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 39 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-268. Input text: ## Vibe Coding - Examples
Source: crawl4ai_vibe_examples_content.llm.md

# ...WebCrawler closed")
        self.ready = False

# --- End Mock ---
```

---
[R_SPLIT_DEBUG] Found 1 local headings: [('Vibe Coding - Examples', 2)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Vibe Coding - Examples', 0)]
[R_SPLIT_DEBUG] Single heading 'Vibe Coding - Examples' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## Vibe Coding - Examples
Source: crawl4ai_vibe_examples_content.llm.md

# ...WebCrawler closed")
        self.ready = False

# --- End Mock ---
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 6. Snippets: ['## Vibe Coding - Examples\nSource: crawl4ai_vibe_examples_content.llm.md\n\n# ...\n**Library Version Context:** 0.6.3\n**Outline Generation Date:** 2024-05-24', '---', 'This document provides a collection of runnable code examples for the `vibe...ed for many examples below, but will be included in each runnable block):**', '```python\nimport asyncio\nimport time\nimport re\nfrom pathlib import Path\nimp...AsyncWebCrawler closed")\n        self.ready = False\n\n# --- End Mock ---\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 6 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-9): ## Vibe Coding - Examples
Source: crawl4ai_vibe_examples_content.llm.md

# ...
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-9. Input text: ## Vibe Coding - Examples
Source: crawl4ai_vibe_examples_content.llm.md

# ...
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
  [R_SPLIT_DEBUG] Found 1 local headings: [('Vibe Coding - Examples', 2)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Vibe Coding - Examples', 0)]
  [R_SPLIT_DEBUG] Single heading 'Vibe Coding - Examples' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (79 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 9-9): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 9-9. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 9-16): This document provides a collection of runnable code examples for the `vibe...ed for many examples below, but will be included in each runnable block):**
  [R_SPLIT_DEBUG] Depth 1: Processing lines 9-16. Input text: This document provides a collection of runnable code examples for the `vibe...ed for many examples below, but will be included in each runnable block):**
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (111 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 16-265): ```python
import asyncio
import time
import re
from pathlib import Path
imp...AsyncWebCrawler closed")
        self.ready = False

# --- End Mock ---
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 16-265. Input text: ```python
import asyncio
import time
import re
from pathlib import Path
imp...AsyncWebCrawler closed")
        self.ready = False

# --- End Mock ---
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ```python
import asyncio
import time
import re
from pathlib import Path
imp...AsyncWebCrawler closed")
        self.ready = False

# --- End Mock ---
```
  [R_SPLIT_DEBUG] Parts from delimiter split: 1. Snippets: ['```python\nimport asyncio\nimport time\nimport re\nfrom pathlib import Path\nimp...AsyncWebCrawler closed")\n        self.ready = False\n\n# --- End Mock ---\n```']
  [R_SPLIT_DEBUG] Delimiter split resulted in no change (1 part equals original text).
  [R_SPLIT_DEBUG] FINAL PRESERVATION CHECK for code block at lines 16-265: '```python
import asyncio
import time
import re
from pathlib import Path
imp...AsyncWebCrawler closed")
        self.ready = False

# --- End Mock ---
```'
  [R_SPLIT_DEBUG] Code block at lines 16-265 is > 2*max_tokens (2800 > 2400). Attempting function split.
[FUNC_SPLIT] Attempting to split code block (length 11713, tokens 2800) at original_start_line 16
[FUNC_SPLIT] Function splitting not fully implemented, returning original block.
  [R_SPLIT_DEBUG] Code block at lines 16-265 is > 2*max_tokens but not effectively split by functions.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 265-267):
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 267-267): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 267-267. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 5 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-392. Input text: ## 8. Advanced Scenarios & Customization

### 8.1. Example: Implementing a ..._crawl.log").exists():
        os.remove("custom_deep_crawl.log")

```

---
[R_SPLIT_DEBUG] Found 7 local headings: [('Advanced Scenarios & Customization', 2), ('Example: Implementing a custom `DeepCrawlStrategy` by subclassing `DeepCrawlStrategy`.', 3), ('Example: Implementing a custom `URLFilter`.', 3), ('Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', 3), ('Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', 3), ('Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', 3), ('Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Advanced Scenarios & Customization', 0)]
[R_SPLIT_DEBUG] Single heading 'Advanced Scenarios & Customization' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 8. Advanced Scenarios & Customization

### 8.1. Example: Implementing a ..._crawl.log").exists():
        os.remove("custom_deep_crawl.log")

```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 15. Snippets: ['## 8. Advanced Scenarios & Customization\n\n### 8.1. Example: Implementing a ...CrawlStrategy`.\nThis provides a skeleton for creating your own crawl logic.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":\n    asyncio.run(custom_deep_crawl_strategy_example())\n```', '### 8.2. Example: Implementing a custom `URLFilter`.\n`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(custom_url_filter_example())\n```', '### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.\nSubclass `URLScorer` and implement the `score` method.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(custom_url_scorer_example())\n```', '### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":\n    asyncio.run(deep_crawl_large_site_efficiently())\n```', "### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.", '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":\n    asyncio.run(deep_crawl_with_llm_extraction())\n```', '### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo..._deep_crawl.log").exists():\n        os.remove("custom_deep_crawl.log")\n\n```', '', '---', '']
[R_SPLIT_DEBUG] Delimiter split resulted in 15 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-5): ## 8. Advanced Scenarios & Customization

### 8.1. Example: Implementing a ...CrawlStrategy`.
This provides a skeleton for creating your own crawl logic.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-5. Input text: ## 8. Advanced Scenarios & Customization

### 8.1. Example: Implementing a ...CrawlStrategy`.
This provides a skeleton for creating your own crawl logic.
  [R_SPLIT_DEBUG] Found 2 local headings: [('Advanced Scenarios & Customization', 2), ('Example: Implementing a custom `DeepCrawlStrategy` by subclassing `DeepCrawlStrategy`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Advanced Scenarios & Customization', 0)]
  [R_SPLIT_DEBUG] Single heading 'Advanced Scenarios & Customization' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (49 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 5-103): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(custom_deep_crawl_strategy_example())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 5-103. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(custom_deep_crawl_strategy_example())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1177 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 103-108): ### 8.2. Example: Implementing a custom `URLFilter`.
`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 103-108. Input text: ### 8.2. Example: Implementing a custom `URLFilter`.
`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Implementing a custom `URLFilter`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Implementing a custom `URLFilter`.', 105)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Implementing a custom `URLFilter`.' (lines 105-108): ### 8.2. Example: Implementing a custom `URLFilter`.
`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 105-108. Input text: ### 8.2. Example: Implementing a custom `URLFilter`.
`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Implementing a custom `URLFilter`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Implementing a custom `URLFilter`.', 105)]
    [R_SPLIT_DEBUG] Single heading 'Example: Implementing a custom `URLFilter`.' at start_line 105. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (80 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 8.2. Example: Implementing a custom `URLFilter`.\n`URLFilter` itself is ... base might be an option if one is provided or creating your own structure.', h='Example: Implementing a custom `URLFilter`.', lines 105-108)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 108-147): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(custom_url_filter_example())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 108-147. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(custom_url_filter_example())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (421 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 147-152): ### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.
Subclass `URLScorer` and implement the `score` method.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 147-152. Input text: ### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.
Subclass `URLScorer` and implement the `score` method.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', 149)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.' (lines 149-152): ### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.
Subclass `URLScorer` and implement the `score` method.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 149-152. Input text: ### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.
Subclass `URLScorer` and implement the `score` method.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', 149)]
    [R_SPLIT_DEBUG] Single heading 'Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.' at start_line 149. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (41 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.\nSubclass `URLScorer` and implement the `score` method.', h='Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.', lines 149-152)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 152-198): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(custom_url_scorer_example())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 152-198. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(custom_url_scorer_example())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (518 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 198-203): ### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 198-203. Input text: ### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', 200)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.' (lines 200-203): ### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 200-203. Input text: ### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', 200)]
    [R_SPLIT_DEBUG] Single heading 'Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.' at start_line 200. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (64 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 8.4. Example: Deep crawling a site with very large number of pages effi... is crucial for very large crawls to manage memory and get feedback sooner.', h='Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.', lines 200-203)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 203-248): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":
    asyncio.run(deep_crawl_large_site_efficiently())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 203-248. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":
    asyncio.run(deep_crawl_large_site_efficiently())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (472 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 248-253): ### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 248-253. Input text: ### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', 250)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.' (lines 250-253): ### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 250-253. Input text: ### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', 250)]
    [R_SPLIT_DEBUG] Single heading 'Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.' at start_line 250. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (56 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to e...sfully crawled page's content is then passed to an `LLMExtractionStrategy`.', h='Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.', lines 250-253)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 253-314): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(deep_crawl_with_llm_extraction())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 253-314. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(deep_crawl_with_llm_extraction())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (691 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 314-319): ### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 314-319. Input text: ### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', 316)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.' (lines 316-319): ### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 316-319. Input text: ### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', 316)]
    [R_SPLIT_DEBUG] Single heading 'Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.' at start_line 316. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (54 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 8.6. Example: Scenario for using `can_process_url` within a strategy to...rategy to implement dynamic filtering logic based on URL and current depth.', h='Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.', lines 316-319)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 319-388): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._deep_crawl.log").exists():
        os.remove("custom_deep_crawl.log")

```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 319-388. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._deep_crawl.log").exists():
        os.remove("custom_deep_crawl.log")

```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (870 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 388-390):
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 390-390): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 390-390. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 390-391):
[R_SPLIT_DEBUG] Returning 13 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-288. Input text: ## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples

`DFSDeePCrawlSt...= "__main__":
    asyncio.run(dfs_with_domain_filter_subdomains())
```

---
[R_SPLIT_DEBUG] Found 9 local headings: [('Depth-First Search (`DFSDeePCrawlStrategy`) Examples', 2), ('Example: Basic `DFSDeePCrawlStrategy` with default depth.', 3), ('Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', 3), ('Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', 3), ('Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', 3), ('Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', 3), ('Example: `DFSDeePCrawlStrategy` - Streaming results.', 3), ('Example: `DFSDeePCrawlStrategy` - Batch results.', 3), ('Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Depth-First Search (`DFSDeePCrawlStrategy`) Examples', 0)]
[R_SPLIT_DEBUG] Single heading 'Depth-First Search (`DFSDeePCrawlStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples

`DFSDeePCrawlSt...= "__main__":
    asyncio.run(dfs_with_domain_filter_subdomains())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 18. Snippets: ['## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples\n\n`DFSDeePCrawlSt...lt `max_depth` for `DFSDeePCrawlStrategy` is typically 10 if not specified.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...h\')}")\n\nif __name__ == "__main__":\n    asyncio.run(dfs_default_depth())\n```', '### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.\nSet `max_depth` to 2 for a DFS crawl.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...cess)\n\n\nif __name__ == "__main__":\n    asyncio.run(dfs_set_max_depth())\n```', '### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.\nLimit the total number of pages crawled by DFS to 3.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3\n\nif __name__ == "__main__":\n    asyncio.run(dfs_set_max_pages())\n```', '### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."\n\nif __name__ == "__main__":\n    asyncio.run(dfs_include_external())\n```', '### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."\n\nif __name__ == "__main__":\n    asyncio.run(dfs_exclude_external())\n```', '### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...)\n\n\nif __name__ == "__main__":\n    asyncio.run(dfs_streaming_results())\n```', '### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...h\')}")\n\nif __name__ == "__main__":\n    asyncio.run(dfs_batch_results())\n```', "### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.", '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":\n    asyncio.run(dfs_with_domain_filter_subdomains())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 18 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-7): ## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples

`DFSDeePCrawlSt...lt `max_depth` for `DFSDeePCrawlStrategy` is typically 10 if not specified.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-7. Input text: ## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples

`DFSDeePCrawlSt...lt `max_depth` for `DFSDeePCrawlStrategy` is typically 10 if not specified.
  [R_SPLIT_DEBUG] Found 2 local headings: [('Depth-First Search (`DFSDeePCrawlStrategy`) Examples', 2), ('Example: Basic `DFSDeePCrawlStrategy` with default depth.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Depth-First Search (`DFSDeePCrawlStrategy`) Examples', 0)]
  [R_SPLIT_DEBUG] Single heading 'Depth-First Search (`DFSDeePCrawlStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (82 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 7-34): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(dfs_default_depth())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 7-34. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(dfs_default_depth())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (267 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 34-39): ### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.
Set `max_depth` to 2 for a DFS crawl.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 34-39. Input text: ### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.
Set `max_depth` to 2 for a DFS crawl.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', 36)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.' (lines 36-39): ### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.
Set `max_depth` to 2 for a DFS crawl.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 36-39. Input text: ### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.
Set `max_depth` to 2 for a DFS crawl.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', 36)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.' at start_line 36. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (43 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.\nSet `max_depth` to 2 for a DFS crawl.', h='Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.', lines 36-39)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 39-66): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...cess)


if __name__ == "__main__":
    asyncio.run(dfs_set_max_depth())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 39-66. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...cess)


if __name__ == "__main__":
    asyncio.run(dfs_set_max_depth())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (243 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 66-71): ### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.
Limit the total number of pages crawled by DFS to 3.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 66-71. Input text: ### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.
Limit the total number of pages crawled by DFS to 3.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', 68)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.' (lines 68-71): ### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.
Limit the total number of pages crawled by DFS to 3.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 68-71. Input text: ### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.
Limit the total number of pages crawled by DFS to 3.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', 68)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.' at start_line 68. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (43 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the...total number of pages.\nLimit the total number of pages crawled by DFS to 3.', h='Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.', lines 68-71)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 71-101): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3

if __name__ == "__main__":
    asyncio.run(dfs_set_max_pages())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 71-101. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3

if __name__ == "__main__":
    asyncio.run(dfs_set_max_pages())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (259 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 101-105): ### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 101-105. Input text: ### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', 103)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.' (lines 103-105): ### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 103-105. Input text: ### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', 103)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.' at start_line 103. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (26 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', h='Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.', lines 103-105)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 105-139): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."

if __name__ == "__main__":
    asyncio.run(dfs_include_external())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 105-139. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."

if __name__ == "__main__":
    asyncio.run(dfs_include_external())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (288 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 139-143): ### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 139-143. Input text: ### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', 141)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.' (lines 141-143): ### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 141-143. Input text: ### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', 141)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.' at start_line 141. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (28 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', h='Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.', lines 141-143)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 143-176): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."

if __name__ == "__main__":
    asyncio.run(dfs_exclude_external())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 143-176. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."

if __name__ == "__main__":
    asyncio.run(dfs_exclude_external())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (272 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 176-180): ### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 176-180. Input text: ### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Streaming results.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Streaming results.', 178)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Streaming results.' (lines 178-180): ### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 178-180. Input text: ### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Streaming results.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Streaming results.', 178)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Streaming results.' at start_line 178. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (20 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.', h='Example: `DFSDeePCrawlStrategy` - Streaming results.', lines 178-180)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 180-208): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)


if __name__ == "__main__":
    asyncio.run(dfs_streaming_results())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 180-208. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...)


if __name__ == "__main__":
    asyncio.run(dfs_streaming_results())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (243 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 208-212): ### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 208-212. Input text: ### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Batch results.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Batch results.', 210)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Batch results.' (lines 210-212): ### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 210-212. Input text: ### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Batch results.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Batch results.', 210)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Batch results.' at start_line 210. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (20 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.', h='Example: `DFSDeePCrawlStrategy` - Batch results.', lines 210-212)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 212-239): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(dfs_batch_results())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 212-239. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(dfs_batch_results())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (236 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 239-244): ### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 239-244. Input text: ### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', 241)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.' (lines 241-244): ### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 241-244. Input text: ### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', 241)]
    [R_SPLIT_DEBUG] Single heading 'Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.' at start_line 241. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (59 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with...s MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.', h='Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.', lines 241-244)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 15 (lines approx 244-285): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":
    asyncio.run(dfs_with_domain_filter_subdomains())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 244-285. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...e__ == "__main__":
    asyncio.run(dfs_with_domain_filter_subdomains())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (420 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 16 (lines approx 285-287):
[R_SPLIT_DEBUG] Recursing on delimiter part 17 (lines approx 287-287): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 287-287. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 17 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-457. Input text: ## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`

Sc..._name__ == "__main__":
    asyncio.run(composite_scorer_nesting())
```

---
[R_SPLIT_DEBUG] Found 20 local headings: [('Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`', 2), ('`KeywordRelevanceScorer`', 3), ('Example: `KeywordRelevanceScorer` with a list of keywords and default weight.', 4), ('Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', 4), ('Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', 4), ('`PathDepthScorer`', 3), ('Example: `PathDepthScorer` with default behavior (penalizing deeper paths).', 4), ('Example: `PathDepthScorer` with custom `depth_penalty_factor`.', 4), ('Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', 4), ('`ContentTypeScorer`', 3), ('Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.', 4), ('Example: `ContentTypeScorer` with custom `content_type_weights`.', 4), ('`DomainAuthorityScorer`', 3), ('Example: Setting up `DomainAuthorityScorer` (conceptual, as DA often requires an external API or dataset).', 4), ('`FreshnessScorer`', 3), ('Example: Setting up `FreshnessScorer` (conceptual, as freshness often requires parsing dates from content or headers).', 4), ('`CompositeScorer`', 3), ('Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.', 4), ('Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', 4), ('Example: Nesting `CompositeScorer` for more complex scoring logic.', 4)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`', 0)]
[R_SPLIT_DEBUG] Single heading 'Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`

Sc..._name__ == "__main__":
    asyncio.run(composite_scorer_nesting())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 28. Snippets: ['## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`\n\nSc...ample: `KeywordRelevanceScorer` with a list of keywords and default weight.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":\n    asyncio.run(scorer_keyword_default_weight())\n```', '#### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(scorer_keyword_custom_weight())\n```', '#### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":\n    asyncio.run(scorer_keyword_case_sensitive())\n```', '### 6.2. `PathDepthScorer`\n\n#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(scorer_path_depth_default())\n```', '#### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...me__ == "__main__":\n    asyncio.run(scorer_path_depth_custom_penalty())\n```', '#### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(scorer_path_depth_favor_deep())\n```', '### 6.3. `ContentTypeScorer`\n\n#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":\n    asyncio.run(scorer_content_type_html_vs_pdf())\n```', '#### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":\n    asyncio.run(scorer_content_type_custom_weights())\n```', '### 6.4. `DomainAuthorityScorer`\n\n#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.', '```python\nimport asyncio\nfrom crawl4ai import DomainAuthorityScorer\n\nasync ..._name__ == "__main__":\n    asyncio.run(setup_domain_authority_scorer())\n```', '### 6.5. `FreshnessScorer`\n\n#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.', '```python\nimport asyncio\nfrom crawl4ai import FreshnessScorer\nfrom datetime...\n\n\nif __name__ == "__main__":\n    asyncio.run(setup_freshness_scorer())\n```', '### 6.6. `CompositeScorer`\n\n#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":\n    asyncio.run(composite_scorer_equal_weights())\n```', '#### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":\n    asyncio.run(composite_scorer_different_weights())\n```', '#### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...\nif __name__ == "__main__":\n    asyncio.run(composite_scorer_nesting())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 28 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-8): ## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`

Sc...ample: `KeywordRelevanceScorer` with a list of keywords and default weight.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-8. Input text: ## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`

Sc...ample: `KeywordRelevanceScorer` with a list of keywords and default weight.
  [R_SPLIT_DEBUG] Found 3 local headings: [('Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`', 2), ('`KeywordRelevanceScorer`', 3), ('Example: `KeywordRelevanceScorer` with a list of keywords and default weight.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`', 0)]
  [R_SPLIT_DEBUG] Single heading 'Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (83 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 8-28): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(scorer_keyword_default_weight())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 8-28. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(scorer_keyword_default_weight())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (272 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 28-32): #### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 28-32. Input text: #### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', 30)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.' (lines 30-32): #### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 30-32. Input text: #### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', 30)]
    [R_SPLIT_DEBUG] Single heading 'Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.' at start_line 30. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (28 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', h='Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.', lines 30-32)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 32-56): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(scorer_keyword_custom_weight())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 32-56. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(scorer_keyword_custom_weight())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (348 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 56-60): #### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 56-60. Input text: #### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', 58)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `KeywordRelevanceScorer` with `case_sensitive=True`.' (lines 58-60): #### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 58-60. Input text: #### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', 58)]
    [R_SPLIT_DEBUG] Single heading 'Example: `KeywordRelevanceScorer` with `case_sensitive=True`.' at start_line 58. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (23 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', h='Example: `KeywordRelevanceScorer` with `case_sensitive=True`.', lines 58-60)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 60-97): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(scorer_keyword_case_sensitive())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 60-97. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._name__ == "__main__":
    asyncio.run(scorer_keyword_case_sensitive())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (576 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 97-104): ### 6.2. `PathDepthScorer`

#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 97-104. Input text: ### 6.2. `PathDepthScorer`

#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).
  [R_SPLIT_DEBUG] Found 2 local headings: [('`PathDepthScorer`', 3), ('Example: `PathDepthScorer` with default behavior (penalizing deeper paths).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`PathDepthScorer`', 99)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`PathDepthScorer`' (lines 99-104): ### 6.2. `PathDepthScorer`

#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 99-104. Input text: ### 6.2. `PathDepthScorer`

#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).
    [R_SPLIT_DEBUG] Found 2 local headings: [('`PathDepthScorer`', 3), ('Example: `PathDepthScorer` with default behavior (penalizing deeper paths).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`PathDepthScorer`', 99)]
    [R_SPLIT_DEBUG] Single heading '`PathDepthScorer`' at start_line 99. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (67 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 6.2. `PathDepthScorer`\n\n#### 6.2.1. Example: `PathDepthScorer` with def...orer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).', h='`PathDepthScorer`', lines 99-104)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 104-137): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(scorer_path_depth_default())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 104-137. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(scorer_path_depth_default())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (432 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 137-141): #### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 137-141. Input text: #### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `PathDepthScorer` with custom `depth_penalty_factor`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `PathDepthScorer` with custom `depth_penalty_factor`.', 139)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `PathDepthScorer` with custom `depth_penalty_factor`.' (lines 139-141): #### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 139-141. Input text: #### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `PathDepthScorer` with custom `depth_penalty_factor`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `PathDepthScorer` with custom `depth_penalty_factor`.', 139)]
    [R_SPLIT_DEBUG] Single heading 'Example: `PathDepthScorer` with custom `depth_penalty_factor`.' at start_line 139. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (23 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.', h='Example: `PathDepthScorer` with custom `depth_penalty_factor`.', lines 139-141)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 141-177): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...me__ == "__main__":
    asyncio.run(scorer_path_depth_custom_penalty())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 141-177. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...me__ == "__main__":
    asyncio.run(scorer_path_depth_custom_penalty())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (467 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 177-181): #### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).
  [R_SPLIT_DEBUG] Depth 1: Processing lines 177-181. Input text: #### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', 179)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).' (lines 179-181): #### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).
    [R_SPLIT_DEBUG] Depth 2: Processing lines 179-181. Input text: #### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', 179)]
    [R_SPLIT_DEBUG] Single heading 'Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).' at start_line 179. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (31 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', h='Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).', lines 179-181)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 181-215): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(scorer_path_depth_favor_deep())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 181-215. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(scorer_path_depth_favor_deep())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (454 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 215-221): ### 6.3. `ContentTypeScorer`

#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 215-221. Input text: ### 6.3. `ContentTypeScorer`

#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentTypeScorer`', 3), ('Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentTypeScorer`', 217)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`ContentTypeScorer`' (lines 217-221): ### 6.3. `ContentTypeScorer`

#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 217-221. Input text: ### 6.3. `ContentTypeScorer`

#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`ContentTypeScorer`', 3), ('Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`ContentTypeScorer`', 217)]
    [R_SPLIT_DEBUG] Single heading '`ContentTypeScorer`' at start_line 217. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (39 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 6.3. `ContentTypeScorer`\n\n#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.', h='`ContentTypeScorer`', lines 217-221)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 221-244): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":
    asyncio.run(scorer_content_type_html_vs_pdf())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 221-244. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ame__ == "__main__":
    asyncio.run(scorer_content_type_html_vs_pdf())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (314 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 244-248): #### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 244-248. Input text: #### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `ContentTypeScorer` with custom `content_type_weights`.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `ContentTypeScorer` with custom `content_type_weights`.', 246)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `ContentTypeScorer` with custom `content_type_weights`.' (lines 246-248): #### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 246-248. Input text: #### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `ContentTypeScorer` with custom `content_type_weights`.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `ContentTypeScorer` with custom `content_type_weights`.', 246)]
    [R_SPLIT_DEBUG] Single heading 'Example: `ContentTypeScorer` with custom `content_type_weights`.' at start_line 246. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (22 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.', h='Example: `ContentTypeScorer` with custom `content_type_weights`.', lines 246-248)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 15 (lines approx 248-287): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(scorer_content_type_custom_weights())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 248-287. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(scorer_content_type_custom_weights())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (545 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 16 (lines approx 287-294): ### 6.4. `DomainAuthorityScorer`

#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 287-294. Input text: ### 6.4. `DomainAuthorityScorer`

#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`DomainAuthorityScorer`', 3), ('Example: Setting up `DomainAuthorityScorer` (conceptual, as DA often requires an external API or dataset).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`DomainAuthorityScorer`', 289)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`DomainAuthorityScorer`' (lines 289-294): ### 6.4. `DomainAuthorityScorer`

#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 289-294. Input text: ### 6.4. `DomainAuthorityScorer`

#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`DomainAuthorityScorer`', 3), ('Example: Setting up `DomainAuthorityScorer` (conceptual, as DA often requires an external API or dataset).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`DomainAuthorityScorer`', 289)]
    [R_SPLIT_DEBUG] Single heading '`DomainAuthorityScorer`' at start_line 289. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (63 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 6.4. `DomainAuthorityScorer`\n\n#### 6.4.1. Example: Setting up `DomainAu...ntiate and potentially use it, but actual scoring depends on external data.', h='`DomainAuthorityScorer`', lines 289-294)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 17 (lines approx 294-313): ```python
import asyncio
from crawl4ai import DomainAuthorityScorer

async ..._name__ == "__main__":
    asyncio.run(setup_domain_authority_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 294-313. Input text: ```python
import asyncio
from crawl4ai import DomainAuthorityScorer

async ..._name__ == "__main__":
    asyncio.run(setup_domain_authority_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (213 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 18 (lines approx 313-320): ### 6.5. `FreshnessScorer`

#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 313-320. Input text: ### 6.5. `FreshnessScorer`

#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`FreshnessScorer`', 3), ('Example: Setting up `FreshnessScorer` (conceptual, as freshness often requires parsing dates from content or headers).', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`FreshnessScorer`', 315)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`FreshnessScorer`' (lines 315-320): ### 6.5. `FreshnessScorer`

#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 315-320. Input text: ### 6.5. `FreshnessScorer`

#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`FreshnessScorer`', 3), ('Example: Setting up `FreshnessScorer` (conceptual, as freshness often requires parsing dates from content or headers).', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`FreshnessScorer`', 315)]
    [R_SPLIT_DEBUG] Single heading '`FreshnessScorer`' at start_line 315. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (58 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 6.5. `FreshnessScorer`\n\n#### 6.5.1. Example: Setting up `FreshnessScore...xample focuses on instantiation. Actual scoring would need date extraction.', h='`FreshnessScorer`', lines 315-320)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 19 (lines approx 320-346): ```python
import asyncio
from crawl4ai import FreshnessScorer
from datetime...


if __name__ == "__main__":
    asyncio.run(setup_freshness_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 320-346. Input text: ```python
import asyncio
from crawl4ai import FreshnessScorer
from datetime...


if __name__ == "__main__":
    asyncio.run(setup_freshness_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (294 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 20 (lines approx 346-352): ### 6.6. `CompositeScorer`

#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 346-352. Input text: ### 6.6. `CompositeScorer`

#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.
  [R_SPLIT_DEBUG] Found 2 local headings: [('`CompositeScorer`', 3), ('Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`CompositeScorer`', 348)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for '`CompositeScorer`' (lines 348-352): ### 6.6. `CompositeScorer`

#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 348-352. Input text: ### 6.6. `CompositeScorer`

#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.
    [R_SPLIT_DEBUG] Found 2 local headings: [('`CompositeScorer`', 3), ('Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('`CompositeScorer`', 348)]
    [R_SPLIT_DEBUG] Single heading '`CompositeScorer`' at start_line 348. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (47 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 6.6. `CompositeScorer`\n\n#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.', h='`CompositeScorer`', lines 348-352)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 21 (lines approx 352-379): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(composite_scorer_equal_weights())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 352-379. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(composite_scorer_equal_weights())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (378 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 22 (lines approx 379-383): #### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 379-383. Input text: #### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', 381)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.' (lines 381-383): #### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 381-383. Input text: #### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', 381)]
    [R_SPLIT_DEBUG] Single heading 'Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.' at start_line 381. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (27 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', h='Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.', lines 381-383)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 23 (lines approx 383-413): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(composite_scorer_different_weights())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 383-413. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__ == "__main__":
    asyncio.run(composite_scorer_different_weights())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (385 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 24 (lines approx 413-417): #### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 413-417. Input text: #### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Nesting `CompositeScorer` for more complex scoring logic.', 4)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Nesting `CompositeScorer` for more complex scoring logic.', 415)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 4.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: Nesting `CompositeScorer` for more complex scoring logic.' (lines 415-417): #### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 415-417. Input text: #### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: Nesting `CompositeScorer` for more complex scoring logic.', 4)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 4. Found 1 headings at this level: [('Example: Nesting `CompositeScorer` for more complex scoring logic.', 415)]
    [R_SPLIT_DEBUG] Single heading 'Example: Nesting `CompositeScorer` for more complex scoring logic.' at start_line 415. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (23 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('#### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.', h='Example: Nesting `CompositeScorer` for more complex scoring logic.', lines 415-417)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 25 (lines approx 417-454): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(composite_scorer_nesting())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 417-454. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...
if __name__ == "__main__":
    asyncio.run(composite_scorer_nesting())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (476 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 26 (lines approx 454-456):
[R_SPLIT_DEBUG] Recursing on delimiter part 27 (lines approx 456-456): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 456-456. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 27 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-386. Input text: ## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples

`BFSDeePCrawl...e__ == "__main__":
    asyncio.run(bfs_no_depth_limit_max_pages())
```

---
[R_SPLIT_DEBUG] Found 11 local headings: [('Breadth-First Search (`BFSDeePCrawlStrategy`) Examples', 2), ('Example: Basic `BFSDeePCrawlStrategy` with default depth.', 3), ('Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', 3), ('Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', 3), ('Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', 3), ('Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', 3), ('Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', 3), ('Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', 3), ('Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', 3), ('Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', 3), ('Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Breadth-First Search (`BFSDeePCrawlStrategy`) Examples', 0)]
[R_SPLIT_DEBUG] Single heading 'Breadth-First Search (`BFSDeePCrawlStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples

`BFSDeePCrawl...e__ == "__main__":
    asyncio.run(bfs_no_depth_limit_max_pages())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 22. Snippets: ['## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples\n\n`BFSDeePCrawl...n 1 if not specified, meaning it crawls the start URL and its direct links.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...h\')}")\n\nif __name__ == "__main__":\n    asyncio.run(bfs_default_depth())\n```', '### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...ccess)\n\nif __name__ == "__main__":\n    asyncio.run(bfs_set_max_depth())\n```', '### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).\nLimit the crawl to a maximum number of pages, regardless of depth.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3\n\nif __name__ == "__main__":\n    asyncio.run(bfs_set_max_pages())\n```', '### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."\n\nif __name__ == "__main__":\n    asyncio.run(bfs_include_external())\n```', '### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."\n\nif __name__ == "__main__":\n    asyncio.run(bfs_exclude_external())\n```', '### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.\nProcess results as they become available, useful for long crawls.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...")\n\nif __name__ == "__main__":\n    asyncio.run(bfs_streaming_results())\n```', '### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...h\')}")\n\nif __name__ == "__main__":\n    asyncio.run(bfs_batch_results())\n```', '### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":\n    asyncio.run(bfs_with_url_pattern_filter())\n```', "### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.", '```python\nimport asyncio\nimport time\nfrom crawl4ai import AsyncWebCrawler, ...\nif __name__ == "__main__":\n    asyncio.run(bfs_demonstrate_shutdown())\n```', '### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(bfs_no_depth_limit_max_pages())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 22 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-7): ## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples

`BFSDeePCrawl...n 1 if not specified, meaning it crawls the start URL and its direct links.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-7. Input text: ## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples

`BFSDeePCrawl...n 1 if not specified, meaning it crawls the start URL and its direct links.
  [R_SPLIT_DEBUG] Found 2 local headings: [('Breadth-First Search (`BFSDeePCrawlStrategy`) Examples', 2), ('Example: Basic `BFSDeePCrawlStrategy` with default depth.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Breadth-First Search (`BFSDeePCrawlStrategy`) Examples', 0)]
  [R_SPLIT_DEBUG] Single heading 'Breadth-First Search (`BFSDeePCrawlStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (94 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 7-34): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(bfs_default_depth())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 7-34. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(bfs_default_depth())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (261 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 34-39): ### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 34-39. Input text: ### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', 36)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).' (lines 36-39): ### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 36-39. Input text: ### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', 36)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).' at start_line 36. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (76 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control c...means only the start URL. `max_depth=1` means start URL + its direct links.', h='Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).', lines 36-39)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 39-67): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ccess)

if __name__ == "__main__":
    asyncio.run(bfs_set_max_depth())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 39-67. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ccess)

if __name__ == "__main__":
    asyncio.run(bfs_set_max_depth())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (293 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 67-72): ### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).
Limit the crawl to a maximum number of pages, regardless of depth.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 67-72. Input text: ### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).
Limit the crawl to a maximum number of pages, regardless of depth.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', 69)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).' (lines 69-72): ### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).
Limit the crawl to a maximum number of pages, regardless of depth.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 69-72. Input text: ### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).
Limit the crawl to a maximum number of pages, regardless of depth.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', 69)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).' at start_line 69. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (53 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the... pages).\nLimit the crawl to a maximum number of pages, regardless of depth.', h='Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).', lines 69-72)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 72-103): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3

if __name__ == "__main__":
    asyncio.run(bfs_set_max_pages())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 72-103. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...) <= 3

if __name__ == "__main__":
    asyncio.run(bfs_set_max_pages())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (273 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 103-108): ### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 103-108. Input text: ### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', 105)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.' (lines 105-108): ### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 105-108. Input text: ### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', 105)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.' at start_line 105. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (48 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to... crawler to follow links that lead to different domains than the start URL.', h='Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.', lines 105-108)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 108-141): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."

if __name__ == "__main__":
    asyncio.run(bfs_include_external())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 108-141. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...k."

if __name__ == "__main__":
    asyncio.run(bfs_include_external())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (269 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 141-146): ### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 141-146. Input text: ### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', 143)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.' (lines 143-146): ### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 143-146. Input text: ### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', 143)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.' at start_line 143. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (51 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (...lt behavior is to only crawl links within the same domain as the start URL.', h='Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.', lines 143-146)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 146-179): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."

if __name__ == "__main__":
    asyncio.run(bfs_exclude_external())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 146-179. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...s."

if __name__ == "__main__":
    asyncio.run(bfs_exclude_external())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (277 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 179-184): ### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.
Process results as they become available, useful for long crawls.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 179-184. Input text: ### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.
Process results as they become available, useful for long crawls.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', 181)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.' (lines 181-184): ### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.
Process results as they become available, useful for long crawls.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 181-184. Input text: ### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.
Process results as they become available, useful for long crawls.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', 181)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.' at start_line 181. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (43 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `Crawler...m=True)`.\nProcess results as they become available, useful for long crawls.', h='Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.', lines 181-184)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 184-213): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...")

if __name__ == "__main__":
    asyncio.run(bfs_streaming_results())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 184-213. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...")

if __name__ == "__main__":
    asyncio.run(bfs_streaming_results())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (273 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 213-218): ### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 213-218. Input text: ### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', 215)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).' (lines 215-218): ### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 215-218. Input text: ### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', 215)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).' at start_line 215. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (48 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunC...ault behavior is to return all results as a list after the crawl completes.', h='Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).', lines 215-218)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 218-245): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(bfs_batch_results())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 218-245. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...h')}")

if __name__ == "__main__":
    asyncio.run(bfs_batch_results())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (245 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 245-250): ### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 245-250. Input text: ### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', 247)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.' (lines 247-250): ### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 247-250. Input text: ### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', 247)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.' at start_line 247. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (55 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with...o guide the crawler, for instance, to only explore URLs matching `/blog/*`.', h='Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.', lines 247-250)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 15 (lines approx 250-294): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(bfs_with_url_pattern_filter())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 250-294. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(bfs_with_url_pattern_filter())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (451 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 16 (lines approx 294-299): ### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 294-299. Input text: ### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', 296)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.' (lines 296-299): ### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 296-299. Input text: ### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', 296)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.' at start_line 296. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (47 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gr...e how to stop a crawl prematurely using the strategy's `shutdown()` method.', h='Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.', lines 296-299)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 17 (lines approx 299-347): ```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, ...
if __name__ == "__main__":
    asyncio.run(bfs_demonstrate_shutdown())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 299-347. Input text: ```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, ...
if __name__ == "__main__":
    asyncio.run(bfs_demonstrate_shutdown())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (412 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 18 (lines approx 347-352): ### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 347-352. Input text: ### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', 349)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.' (lines 349-352): ### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 349-352. Input text: ### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', 349)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.' at start_line 349. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (59 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` li...limited (or very high) but the crawl stops after a certain number of pages.', h='Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.', lines 349-352)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 19 (lines approx 352-383): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(bfs_no_depth_limit_max_pages())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 352-383. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(bfs_no_depth_limit_max_pages())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (272 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 20 (lines approx 383-385):
[R_SPLIT_DEBUG] Recursing on delimiter part 21 (lines approx 385-385): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 385-385. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 21 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-590. Input text: ## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples

`BestFirs...e__ == "__main__":
    asyncio.run(best_first_batch_size_effect())
```

---
[R_SPLIT_DEBUG] Found 15 local headings: [('Best-First Crawling (`BestFirstCrawlingStrategy`) Examples', 2), ('Example: Basic `BestFirstCrawlingStrategy` with default parameters.', 3), ('Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', 3), ('Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', 3), ('Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', 3), ('Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', 3), ('Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', 3), ('Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', 3), ('Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', 3), ('Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', 3), ('Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', 3), ('Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', 3), ('Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', 3), ('Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', 3), ('Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Best-First Crawling (`BestFirstCrawlingStrategy`) Examples', 0)]
[R_SPLIT_DEBUG] Single heading 'Best-First Crawling (`BestFirstCrawlingStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text over token limit. Calling split_by_markdown_delimiter for: ## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples

`BestFirs...e__ == "__main__":
    asyncio.run(best_first_batch_size_effect())
```

---
[R_SPLIT_DEBUG] Parts from delimiter split: 30. Snippets: ['## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples\n\n`BestFirs...haves somewhat like BFS but might have different internal queue management.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(best_first_default_params())\n```', '### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...ss)\n\nif __name__ == "__main__":\n    asyncio.run(best_first_max_depth())\n```', '### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...= 3\n\nif __name__ == "__main__":\n    asyncio.run(best_first_max_pages())\n```', '### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":\n    asyncio.run(best_first_include_external())\n```', '### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(best_first_keyword_scorer())\n```', '### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.\nThis scorer penalizes deeper paths by default.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(best_first_path_depth_scorer())\n```', '### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":\n    asyncio.run(best_first_content_type_scorer())\n```', '### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":\n    asyncio.run(best_first_composite_scorer())\n```', '### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo..._ == "__main__":\n    asyncio.run(best_first_with_content_type_filter())\n```', '### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":\n    asyncio.run(best_first_streaming_order())\n```', '### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":\n    asyncio.run(best_first_batch_analysis())\n```', '### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.\nThis explicitly shows how to get these specific metadata fields.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":\n    asyncio.run(best_first_access_metadata())\n```', '### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', '```python\nimport asyncio\nimport time\nfrom crawl4ai import AsyncWebCrawler, ...ame__ == "__main__":\n    asyncio.run(best_first_demonstrate_shutdown())\n```', '### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.', '```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":\n    asyncio.run(best_first_batch_size_effect())\n```', '', '---']
[R_SPLIT_DEBUG] Delimiter split resulted in 30 parts. Recursing on each (single heading guard).
[R_SPLIT_DEBUG] Recursing on delimiter part 0 (lines approx 0-7): ## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples

`BestFirs...haves somewhat like BFS but might have different internal queue management.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 0-7. Input text: ## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples

`BestFirs...haves somewhat like BFS but might have different internal queue management.
  [R_SPLIT_DEBUG] Found 2 local headings: [('Best-First Crawling (`BestFirstCrawlingStrategy`) Examples', 2), ('Example: Basic `BestFirstCrawlingStrategy` with default parameters.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Best-First Crawling (`BestFirstCrawlingStrategy`) Examples', 0)]
  [R_SPLIT_DEBUG] Single heading 'Best-First Crawling (`BestFirstCrawlingStrategy`) Examples' at start_line 0. Processing as a single block with this heading.
  [R_SPLIT_DEBUG] Text under token limit (90 <= 1200). Returning as single chunk with heading.
[R_SPLIT_DEBUG] Recursing on delimiter part 1 (lines approx 7-32): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_default_params())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 7-32. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_default_params())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (255 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 2 (lines approx 32-36): ### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 32-36. Input text: ### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', 34)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.' (lines 34-36): ### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 34-36. Input text: ### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', 34)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.' at start_line 34. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (27 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', h='Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.', lines 34-36)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 3 (lines approx 36-62): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ss)

if __name__ == "__main__":
    asyncio.run(best_first_max_depth())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 36-62. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...ss)

if __name__ == "__main__":
    asyncio.run(best_first_max_depth())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (283 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 4 (lines approx 62-66): ### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 62-66. Input text: ### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', 64)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.' (lines 64-66): ### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 64-66. Input text: ### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', 64)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.' at start_line 64. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (28 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', h='Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.', lines 64-66)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 5 (lines approx 66-96): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...= 3

if __name__ == "__main__":
    asyncio.run(best_first_max_pages())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 66-96. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...= 3

if __name__ == "__main__":
    asyncio.run(best_first_max_pages())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (266 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 6 (lines approx 96-100): ### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 96-100. Input text: ### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', 98)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.' (lines 98-100): ### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 98-100. Input text: ### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', 98)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.' at start_line 98. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (23 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', h='Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.', lines 98-100)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 7 (lines approx 100-134): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(best_first_include_external())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 100-134. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(best_first_include_external())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (304 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 8 (lines approx 134-138): ### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 134-138. Input text: ### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', 136)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.' (lines 136-138): ### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 136-138. Input text: ### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', 136)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.' at start_line 136. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (32 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', h='Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.', lines 136-138)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 9 (lines approx 138-177): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_keyword_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 138-177. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_keyword_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (433 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 10 (lines approx 177-182): ### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.
This scorer penalizes deeper paths by default.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 177-182. Input text: ### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.
This scorer penalizes deeper paths by default.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', 179)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.' (lines 179-182): ### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.
This scorer penalizes deeper paths by default.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 179-182. Input text: ### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.
This scorer penalizes deeper paths by default.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', 179)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.' at start_line 179. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (42 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to ...ity based on URL path depth.\nThis scorer penalizes deeper paths by default.', h='Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.', lines 179-182)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 11 (lines approx 182-223): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(best_first_path_depth_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 182-223. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(best_first_path_depth_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (463 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 12 (lines approx 223-227): ### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 223-227. Input text: ### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', 225)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.' (lines 225-227): ### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 225-227. Input text: ### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', 225)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.' at start_line 225. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (31 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', h='Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.', lines 225-227)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 13 (lines approx 227-269): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(best_first_content_type_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 227-269. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...name__ == "__main__":
    asyncio.run(best_first_content_type_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (480 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 14 (lines approx 269-273): ### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 269-273. Input text: ### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', 271)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.' (lines 271-273): ### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 271-273. Input text: ### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', 271)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.' at start_line 271. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (39 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', h='Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.', lines 271-273)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 15 (lines approx 273-308): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(best_first_composite_scorer())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 273-308. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo... __name__ == "__main__":
    asyncio.run(best_first_composite_scorer())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (373 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 16 (lines approx 308-312): ### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 308-312. Input text: ### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', 310)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.' (lines 310-312): ### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 310-312. Input text: ### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', 310)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.' at start_line 310. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (34 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', h='Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.', lines 310-312)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 17 (lines approx 312-353): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._ == "__main__":
    asyncio.run(best_first_with_content_type_filter())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 312-353. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo..._ == "__main__":
    asyncio.run(best_first_with_content_type_filter())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (433 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 18 (lines approx 353-358): ### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 353-358. Input text: ### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', 355)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.' (lines 355-358): ### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 355-358. Input text: ### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', 355)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.' at start_line 355. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (51 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and obse...s to demonstrate that higher-scored URLs are (generally) processed earlier.', h='Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.', lines 355-358)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 19 (lines approx 358-401): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(best_first_streaming_order())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 358-401. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(best_first_streaming_order())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (457 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 20 (lines approx 401-405): ### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 401-405. Input text: ### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', 403)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.' (lines 403-405): ### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 403-405. Input text: ### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', 403)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.' at start_line 403. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (26 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', h='Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.', lines 403-405)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 21 (lines approx 405-441): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_batch_analysis())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 405-441. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...if __name__ == "__main__":
    asyncio.run(best_first_batch_analysis())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (334 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 22 (lines approx 441-446): ### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.
This explicitly shows how to get these specific metadata fields.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 441-446. Input text: ### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.
This explicitly shows how to get these specific metadata fields.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', 443)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.' (lines 443-446): ### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.
This explicitly shows how to get these specific metadata fields.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 443-446. Input text: ### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.
This explicitly shows how to get these specific metadata fields.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', 443)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.' at start_line 443. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (50 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting...metadata`.\nThis explicitly shows how to get these specific metadata fields.', h='Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.', lines 443-446)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 23 (lines approx 446-482): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(best_first_access_metadata())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 446-482. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...f __name__ == "__main__":
    asyncio.run(best_first_access_metadata())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (339 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 24 (lines approx 482-486): ### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 482-486. Input text: ### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', 484)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.' (lines 484-486): ### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 484-486. Input text: ### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', 484)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.' at start_line 484. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (30 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', h='Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.', lines 484-486)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 25 (lines approx 486-532): ```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, ...ame__ == "__main__":
    asyncio.run(best_first_demonstrate_shutdown())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 486-532. Input text: ```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, ...ame__ == "__main__":
    asyncio.run(best_first_demonstrate_shutdown())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (404 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 26 (lines approx 532-537): ### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.
  [R_SPLIT_DEBUG] Depth 1: Processing lines 532-537. Input text: ### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.
  [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', 3)]
  [R_SPLIT_DEBUG] Branch: Headings found.
  [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', 534)]
  [R_SPLIT_DEBUG] Branch: Splitting by 1 headings of level 3.
  [R_SPLIT_DEBUG] Recursing on heading split for 'Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.' (lines 534-537): ### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.
    [R_SPLIT_DEBUG] Depth 2: Processing lines 534-537. Input text: ### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.
    [R_SPLIT_DEBUG] Found 1 local headings: [('Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', 3)]
    [R_SPLIT_DEBUG] Branch: Headings found.
    [R_SPLIT_DEBUG] Min heading level: 3. Found 1 headings at this level: [('Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', 534)]
    [R_SPLIT_DEBUG] Single heading 'Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.' at start_line 534. Processing as a single block with this heading.
    [R_SPLIT_DEBUG] Text under token limit (92 <= 1200). Returning as single chunk with heading.
  [R_SPLIT_DEBUG] Returning 1 chunks from heading-based splitting. Snippets: ["('### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `...nternal implementation detail of how the strategy uses `crawler.arun_many`.', h='Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.', lines 534-537)"]
[R_SPLIT_DEBUG] Recursing on delimiter part 27 (lines approx 537-587): ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(best_first_batch_size_effect())
```
  [R_SPLIT_DEBUG] Depth 1: Processing lines 537-587. Input text: ```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunCo...__name__ == "__main__":
    asyncio.run(best_first_batch_size_effect())
```
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (598 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Recursing on delimiter part 28 (lines approx 587-589):
[R_SPLIT_DEBUG] Recursing on delimiter part 29 (lines approx 589-589): ---
  [R_SPLIT_DEBUG] Depth 1: Processing lines 589-589. Input text: ---
  [R_SPLIT_DEBUG] No local headings found.
  [R_SPLIT_DEBUG] Branch: No local headings.
  [R_SPLIT_DEBUG] Text under token limit (1 <= 1200). Returning as single chunk.
[R_SPLIT_DEBUG] Returning 29 chunks from delimiter part recursion (single heading guard).
[R_SPLIT_DEBUG] Depth 0: Processing lines 0-112. Input text: ## 1. Introduction to Deep Crawling (`vibe`)

The `vibe` component of Crawl... == "__main__":
    asyncio.run(minimal_single_level_deep_crawl())
```

---
[R_SPLIT_DEBUG] Found 4 local headings: [('Introduction to Deep Crawling (`vibe`)', 2), ('Example: Enabling Basic Deep Crawl with `BFSDeePCrawlStrategy` via `CrawlerRunConfig`.', 3), ('Example: Understanding `CrawlResult.metadata` (depth, parent_url, score) in Deep Crawl Results.', 3), ('Example: Minimal setup for deep crawling a single level deep.', 3)]
[R_SPLIT_DEBUG] Branch: Headings found.
[R_SPLIT_DEBUG] Min heading level: 2. Found 1 headings at this level: [('Introduction to Deep Crawling (`vibe`)', 0)]
[R_SPLIT_DEBUG] Single heading 'Introduction to Deep Crawling (`vibe`)' at start_line 0. Processing as a single block with this heading.
[R_SPLIT_DEBUG] Text under token limit (1200 <= 1200). Returning as single chunk with heading.
Final chunked Markdown files written under 'final_output_structured_examples'. Total files: 78
